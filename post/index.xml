<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on T. Ben Thompson</title><link>https://tbenthompson.com/post/</link><description>Recent content in Posts on T. Ben Thompson</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 14 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://tbenthompson.com/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Gaussian quadrature is not optimal</title><link>https://tbenthompson.com/post/better_than_gauss/</link><pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/better_than_gauss/</guid><description><![CDATA[ It&rsquo;s conventional wisdom that Gaussian quadrature is the most point-efficient quadrature formula for analytic functions. But, it&rsquo;s not true! &ldquo;New quadrature formulas from conformal maps&rdquo; by Hale and Trefethen (2008) demonstrate that it&rsquo;s possible to have quadrature formulas that converge about 50% faster for analytic functions. The paper is quite accessible and I encourage you to read it, but I also wanted to help bring attention to it since it seems like it should be more well-known.]]></description><content:encoded><![CDATA[ <p>It&rsquo;s conventional wisdom that Gaussian quadrature is the most point-efficient quadrature formula for analytic functions. But, it&rsquo;s not true! <a href="http://www.cs.ox.ac.uk/files/731/NA-07-15.pdf">&ldquo;New quadrature formulas from conformal maps&rdquo; by Hale and Trefethen (2008)</a> demonstrate that it&rsquo;s possible to have quadrature formulas that converge about 50% faster for analytic functions. The paper is quite accessible and I encourage you to read it, but I also wanted to help bring attention to it since it seems like it should be more well-known.</p>
<h3 id="polynomials-vs-analytic-functions">Polynomials vs analytic functions</h3>
<p>So, where did the misconception that Gaussian quadrature is the best possible quadrature rule for smooth functions? I think it comes from framing the problem in terms of the maximum order of polynomial that can be integrated exactly. By that metric, Gaussian quadrature <strong>is</strong> the best that can be done. If we write down a quadrature rule like:</p>
<p>\begin{equation}
\int_{-1}^{1} f(x) dx \approx \sum_{k}^n w_k f(x_k)
\end{equation}</p>
<p>then there are $n$ points, $x_k$, and $n$ weights $w_k$. Solving a linear system for the points and weights that exactly integrate a polynomial results in a solvable linear system with $2n$ rows and columns. And a polynomial with order $2n-1$ has $2n$ coefficients, so Gaussian quadrature is clearly optimal in the order of polynomial integrable with $N$ points and weights (this isn&rsquo;t a rigorous proof).</p>
<p>But, it <em>does not follow</em> that the best quadrature rule for small polynomials is also the best possible quadrature rule for general analytic functions. As the Hale and Trefethen paper demonstrates, Gaussian quadrature is suboptimal by approximately a factor of $\pi/2$ in terms of the number of quadrature points required for a given level of error.</p>
<h3 id="just-give-me-a-quadrature-rule">Just give me a quadrature rule</h3>
<p>There are several formulas in the paper, but this one is particularly
\begin{align}
\tilde{I}_n(f) &amp;= \sum_k^n \tilde{w}_k f(\tilde{x}_k) , ~~~ \tilde{x}_k = w_kg'(x_k), ~~~ \tilde{x}_k=g(x_k) \\<br>
g(s) &amp;= \frac{1}{53089}(40320s + 6720s^3 + 3024s^5 + 1800s^7 + 1225s^9)
\end{align}</p>
<h3 id="why-does-this-work">Why does this work?</h3>
<p>In traditional convergence proofs for Gaussian quadrature, we must assume a little bit more than just having an analytic integrand. The standard approach is to assume that $f(x)$ is analytic in an elliptical domain in the complex plane surrounding the real line segment $[-1, 1]$. The left side of the figure below is showing the right-hand half of such an ellipse. The thing to note is that this is an unbalanced regularity requirement:</p>
<ul>
<li>near $x=\pm 1$, $f(x)$ is allowed to have singularities or other non-analytic behavior very nearby.</li>
<li>on the other hand, near $x=0$, $f(x)$ has a very wide domain in which it is analytic.</li>
</ul>
<p>Basically, we&rsquo;re putting much more stringent requirements on the center of the domain than on the end points of the domain. The idea in the Hale and Trefethen is to perform a conformal mapping of this ellipse into a domain that is more balanced. This is demonstrated graphically in the figure below.</p>
<p><img src="https://tbenthompson.com/images/elliptic_gauss.png" alt="ellipse"></p>
<h3 id="does-this-actually-work">Does this actually work?</h3>
<p>For this, I&rsquo;ll just copy the figure from the paper. It depends on the integrand, but for many analytic functions, yes, the new quadrature rule is more point efficient! For further explanation of the successes and failures here, go take a look at the paper!</p>
<p><img src="https://tbenthompson.com/images/better_gauss.png" alt="ellipse"></p>
<h3 id="more-myths">More myths!</h3>
<p>Check out this <a href="https://people.maths.ox.ac.uk/trefethen/mythspaper.pdf">Lloyd Trefethen essay</a> for several other myths about polynomials and numerical methods.</p>
]]></content:encoded></item><item><title>Reflective deep work</title><link>https://tbenthompson.com/post/reflective_deep_work/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/reflective_deep_work/</guid><description> You might also call this introspective deep work, or self-reflective deep work.
I just finished reading Deep Work and it has me thinking about applying a few of the concepts well beyond my career. &amp;ldquo;Deep work&amp;rdquo; is all about making space for the hard intellectual work at the core of most knowledge workers' jobs. But, the concept of making space in your day or week or month for deep thought seems like it should apply well beyond my career.</description><content:encoded><![CDATA[ <p><sub>You might also call this introspective deep work, or self-reflective deep work.</sub></p>
<p>I just finished reading <a href="https://www.goodreads.com/book/show/25744928-deep-work">Deep Work</a> and it has me thinking about applying a few of the concepts well beyond my career. &ldquo;Deep work&rdquo; is all about making space for the hard intellectual work at the core of most knowledge workers' jobs. But, the concept of making space in your day or week or month for deep thought seems like it should apply well beyond my career. Reflective deep work could even be a unifying theme for a lot of self-help or productivity advice. In some parts of life, I operate on auto-pilot almost all the time. If I&rsquo;m brushing my teeth, that&rsquo;s perfect. But, in other situations, it would be better to occasionally stop and think.</p>
<p>Examples:</p>
<ul>
<li>Sitting down with a blank sheet of paper and thinking for an hour about how to improve your relationship with your significant other. Even if the relationship is already great!</li>
<li>Intentional thought and planning about how to enhance my running training and how to solve my running injuries.</li>
<li>Puzzling out better dog training techniques when the current approach isn&rsquo;t working. Or even, the mere realization that the current training approach isn&rsquo;t working.</li>
</ul>
<p>You might be thinking this all sounds too much like a weekly review. I don&rsquo;t disagree! A <a href="https://www.benkuhn.net/weekly/">weekly review</a> is a form of reflective deep work normally focused on <a href="https://nesslabs.com/weekly-review">meta-level career topics</a> or <a href="https://gettingthingsdone.com/wp-content/uploads/2014/10/Weekly_Review_Checklist.pdf">organization</a>. But, I think it&rsquo;s useful to occasionally go beyond the core goals and topics you might consider in a weekly review. I&rsquo;m going to reserve an hour every couple weeks to think hard about something that I wouldn&rsquo;t normally think about. To avoid missing important subjects, I&rsquo;ll spend the first few minutes listing some options for what to think about that session: random thoughts, activities, people, but probably not work.</p>
]]></content:encoded></item><item><title>Why it's okay to share your code</title><link>https://tbenthompson.com/post/share_your_code/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/share_your_code/</guid><description> TL;DR: If you write a paper that involves complex code, share your code freely online. Don&amp;rsquo;t make it pretty. Don&amp;rsquo;t make it easy to use. Refuse to support the code. Just share it!
People have talked a lot already about why it&amp;rsquo;s a good idea to share code with your research. I want to address a specific component of this. The desire to share good, well-maintained code often becomes a barrier to sharing any code at all.</description><content:encoded><![CDATA[ <p>TL;DR: If you write a paper that involves complex code, share your code freely online. Don&rsquo;t make it pretty. Don&rsquo;t make it easy to use. Refuse to support the code. Just share it!</p>
<p>People have talked a lot already about why it&rsquo;s a good idea to share code with your research. I want to address a specific component of this. The desire to share good, well-maintained code often becomes a barrier to sharing any code at all. So, here, I&rsquo;d just like to say that we love you, and we love your code too. No matter what it looks like.</p>
<p>Disclaimer: This mostly comes from my experience in the computational methods and Earth science communities.</p>
<h2 id="no-you-cant-see-my-work">No, you can’t see my work.</h2>
<p>I frequently ask people for the code they used to write a paper. The answer is often a deflection or just &ldquo;No.&rdquo; It&rsquo;s depressing because seeing code is so incredibly useful. With a complex algorithm, I&rsquo;m more likely to jump straight to the code if I can rather than trying to read the paper description carefully. The code is the truth. The writing is just the broad strokes. For technical work, there&rsquo;s often dozens of tiny technical decisions that are never important enough to make it into the final published paper. But, at the same time, those little choices are what make or break an implementation. If I&rsquo;m debugging why my implementation doesn&rsquo;t work, I&rsquo;m going to go read your code. I don&rsquo;t need to be able to run it for this to be valuable. It doesn&rsquo;t even need to be clean or easy to understand. If I care enough, I&rsquo;ll spend hours just sifting through line by line.</p>
<p>Published papers are just a filter on the truth behind your research. In very literal fields like mathematics, the filter is very thin. The proofs and theorems are the truth and can be written literally in paper form. On the far other end, in some fields, the filter is thick. There&rsquo;s no way to share the truth, the lab experiment. The experiment is over. But we can and should share the raw-est results possible and a precise procedure for repeating the experiment. Computational methods and modeling papers are much more on the math side of the spectrum. The truth is straightforwardly sharable in the form of code. But, we don&rsquo;t do it!</p>
<p>So, let’s discuss a couple of the common reasons I&rsquo;ve heard from people that don&rsquo;t share their code!</p>
<h2 id="embarrassed-about-the-quality-and-usability">Embarrassed about the quality and usability.</h2>
<p>These folks aren&rsquo;t even sure the code they share will run at all or produce the results they presented in their paper. They had a good idea and slammed together something that demonstrated that their idea works well enough to publish a paper.</p>
<p>For these people, I think we need to encourage a culture of acceptance. It’s okay to produce ugly and unusable code. I’ve done it. You’ve done it. We don’t all have the skills to make nice software. It’s even rarer to find those skills in someone who has spent years practicing some other skillset like doing good research! We don’t always have the time to write something reusable and we shouldn’t be expected to. When the bar for publishable work is too high, the result is fewer publications and a longer publication cycle and that’s bad for everyone involved! Apparently the bar for publishing code is so high that we often don’t even do it at all. So I’d like to try going to the opposite extreme and explicitly setting the bar so low that we publish all the code all the time.</p>
<h2 id="worried-about-the-maintenance-burden">Worried about the maintenance burden.</h2>
<p>These folks think they will have to maintain the code or support users. For these people, I have a simple message. There is no maintenance burden! You can and should say no. In fact, you’re already quite good at saying no when I email you asking for your code. So, just do it the other way around. Share your code freely and then say no when people ask for help. Unless you want to help. That’s always an option. But don’t feel obligated. The code is the truth and if someone care enough, they can dig through and figure out what they need to on their own.</p>
<h2 id="all-the-other-reasons-that-i-wont-address-here">All the other reasons that I won&rsquo;t address here</h2>
<p>There are a lot of other reasons someone would decline to share their code:</p>
<ul>
<li>Career incentives. They think their secret sauce code is their ticket to a job/tenure/grants/etc. Bummer!</li>
<li>The code doesn&rsquo;t exist. Data analysis was done using an interactive system and the original source is gone. Scary!</li>
<li>Proprietary code. But then why is the paper not proprietary? Dang!</li>
<li>The code only runs on (insert supercomputer here). See above. Being able to just read the code is very valuable.</li>
</ul>
<p>I&rsquo;m not going to dive into these here, but it&rsquo;s worth thinking about how to train people and redesign incentive systems so that these are no longer barriers.</p>
<h2 id="some-useful-links">Some useful links:</h2>
<ul>
<li><a href="https://faculty.washington.edu/rjl/pubs/topten/topten.pdf">Randall LeVeque made a more extensive and better argument than me!</a></li>
<li><a href="https://www.paperswithcode.com/">Papers with Code</a> is a great project linking papers and their code repositories. It&rsquo;s focused on the machine learning world. It would be great to have something similar in other areas.</li>
<li><a href="https://github.com/paperswithcode/releasing-research-code">This set of guidelines</a> for releasing machine learning research code.</li>
<li><a href="http://rescience.github.io/">ReScience C</a> is an open-access journal dedicated to reproduction of computational papers.</li>
</ul>
<p><em>Thanks to Greg Wagner, Liz Santorella for helping out. If you want to chat please <a href="mailto:t.ben.thompson@gmail.com">get in touch</a>!</em></p>
]]></content:encoded></item><item><title>Maintaining momentum</title><link>https://tbenthompson.com/post/maintaining_momentum/</link><pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/maintaining_momentum/</guid><description><![CDATA[ Collaboration in an unstructured world While I haven&rsquo;t made any decisions yet, I&rsquo;m currently considering heading off into a much less structured career path which could be variously described as &ldquo;independent scientist&rdquo;, &ldquo;entrepreneuer&rdquo;, &ldquo;contractor&rdquo; or &ldquo;self-employed open-source developer&rdquo;. Along those lines, I&rsquo;ve been wondering what makes work good for me and how I can put the pieces together reliably. I think the critical piece will be to proactively communicate with lots of people.]]></description><content:encoded><![CDATA[ <h3 id="collaboration-in-an-unstructured-world">Collaboration in an unstructured world</h3>
<p>While I haven&rsquo;t made any decisions yet, I&rsquo;m currently considering heading off into a much less structured career path which could be variously described as &ldquo;independent scientist&rdquo;, &ldquo;entrepreneuer&rdquo;, &ldquo;contractor&rdquo; or &ldquo;self-employed open-source developer&rdquo;. Along those lines, I&rsquo;ve been wondering what makes work good for me and how I can put the pieces together reliably. I think the critical piece will be to proactively communicate with lots of people.</p>
<p>At the moment, I work as a machine learning engineer for QuantCo, working together with business people, economists, statisticians and other software engineers to build useful software systems for things like ecommerce pricing or insurance fraud detection. Jobs like this are inherently collaborative. Half the job is to work together with other people to figure out what exactly you should be doing and how to do it. There are strategy discussions, design discussions, pair programming sessions, stand ups and just plan old hang-out-and-chats.</p>
<p>By comparison, as an independent scientist, there&rsquo;s not necessarily anyone there that is depending on me, helping me or working together with me. Even as a faculty member within the structure of academia, there&rsquo;s a lot of individualism to science. A single research group is often quite disconnected from the rest of the community. Papers aren&rsquo;t normally communicated until they&rsquo;re done. A lot of people simply don&rsquo;t care about the work you&rsquo;re doing. Seminars or conferences can be confrontational. People don&rsquo;t often say what they really think about some research outside of their small social network.</p>
<p>So, as I&rsquo;ve been considering this potential path, I&rsquo;ve been thinking a lot about how to establish the kind of collaboration I want. And I think part of the solution is to really focus on the people. While the true goal of my work is scientific progress combined with personal fulfullment, a reasonable proxy goal might be to make it all about people and engagement: people that value/use my work, people that talk to me, people that work with me. I doubt the end result will be very different between these objective functions and the people-focused version is going to do a better job of maintaining my momentum.</p>
<p>Along those lines, these are some rough ideas I&rsquo;ve had:</p>
<ul>
<li>Find one or more close collaborators. This is my number one collaborative goal &ndash; to find someone who is interested in working together on a daily basis. Not your typical weekly arms-length collaboration. I don&rsquo;t know if this goal is achievable.</li>
<li>Do projects <strong>for</strong> people. Facilitate other people&rsquo;s work as much as I can. Make it undeniably valuable for other people to seek me out and work with me.</li>
<li>Write a lot. I probably won&rsquo;t be publishing articles in journals, but even if I were, the 6-12 month publishing cadence is far too slow to rely completely on publications for communicating with other scientists. I&rsquo;d prefer a feedback loop on the order of a week or two. I plan to use my website for my technical writing.</li>
<li>Email people on a regular basis. Cold emailing people. Build an email list that I use any time I write a new website post. Recruit hard for my email list. Should I become the guy-who-spams-his-website-posts-to-everyone? Is this a case of no publicity is bad publicity? I&rsquo;m not sure.</li>
<li>Lots of video calls! Visit in person once the pandemic is over if people are in the northeast.</li>
</ul>
<h3 id="three-pillars-of-good-work">Three pillars of good work</h3>
<p>While discussing some career ideas with a friend recently, she asked me to back up and think about what basic components make work good and productive for me. Below was my attempt at a response:</p>
<ol>
<li>A medium collaboration level. Maybe ~10-40% of time is spent communicating and working together with good people who are invested in the work I&rsquo;m doing and I&rsquo;m invested in their work.</li>
<li>Work that is intrinsically motivating. This is a fairly broad category. That could be building something that I think people want, could be fixing something that is broken, could be learning something new.</li>
<li>Enough control. I don&rsquo;t need to be master of the universe, but without some control, people will treat you poorly, projects will get cancelled and any number of other bad things will happen.</li>
</ol>
<p>I think I&rsquo;ve had two of these things at many points and that was super motivating and resulted in high happiness and high productivity:</p>
<ul>
<li>During grad school, early on, I had instrinsically motivating work (#2) and plenty of control (#3). That was really awesome. I lost some of both due to some of the mistakes I made early on and getting stuck by the need to finish my dissertation when I also knew I was sort of working on a dead-end research-wise that very few people would be interested in. In the absence of organizational requirements like submitting a dissertation, I would&rsquo;ve made a huge pivot in direction around year 4 or so. Maybe I should have made that pivot anyway, but I&rsquo;m not sure. More collaboration (#1) would&rsquo;ve have helped a lot in a couple critical points to help me avoid the mistakes I made.</li>
<li>I also had #2 and #3 when I was in high school playing with deep learning. Yeah, deep learning! Way back in 2007! I was super motivated and implemented some cool autoencoder stuff. I just wasn&rsquo;t enough of an adult to continue down that path. I needed to have a lot of collaboration with people with more experience and I had none. I needed several years more math skills. I needed to go get in touch with some faculty or grad students working on this stuff. So, this project fell short of its potential due to a lack of collaboration (#1).</li>
<li>While working at QuantCo, at various points, I&rsquo;ve had #1 and #2. Last year, I was working on improving a forecasting machine learning system at client-name-removed and that definitely qualified as #2 and I had enough interaction with other team members and client stakeholders that I definitely had #1. It was pretty great and I was super motivated. The project was shut down so this ended because I didn&rsquo;t have control (#3).</li>
<li>I had #1 and #2 when I worked as a software engineer for a web-based therapy records system called TherapyCharts the summer after high school. Fast, dynamic work in an office with the founders. I continued to work on TherapyCharts during college on and off for a couple years as a part-time contractor. But, I had much less motivation. It had become clear to me that the company was not going to be successful and I felt like I needed to jump ship. But, maybe more important was that it had become much less collaborative because I was several hundred miles away and I was only communicating with the founders every few days or once a week.</li>
</ul>
<p>I took away two lessons from this list. First, when I&rsquo;ve felt unfocused or unable to get good work done, it was because something was fundamentally <em>wrong</em> and not because I needed to push harder. You can&rsquo;t push through a fundamentally flawed mathematical method or a flawed business plan. When things went from going great to going badly in the situations above, I probably should&rsquo;ve made changes instead of just languishing or pushing against a brick wall. Second, any time I can think of where I had two of these things was a really good time for my motivation and happiness. I don&rsquo;t know what would happen if I had all three of these things and maintained that situation for a few years. Probably something really good. I should probably try to find that!</p>
<p><em>Thanks to Liz Santorella, Ruth Byers, Brendan Meade, Reena Joubert, and Phoebe DeVries for helping me crystallize some of the ideas in this post and for providing some edits. The ideas still are pretty rough and hypothetical, so if you want to chat please <a href="mailto:t.ben.thompson@gmail.com">get in touch</a>!</em></p>
]]></content:encoded></item><item><title>Maybe we should stop using planar triangles for fault modeling</title><link>https://tbenthompson.com/post/surface_representation/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/surface_representation/</guid><description> Let&amp;rsquo;s consider a model problem where the goal is to calculate the stress on a fault due to some slip on that fault. We also include a topographic free surface. How should we model this surface?
Should we use planar triangles to model a curved fault? What happens at the junction between two triangles? If we do the natural fault thing and require the slip to be tangential to the suface, the slip vector has a step function at the junction between the two triangles.</description><content:encoded><![CDATA[ <p>Let&rsquo;s consider a model problem where the goal is to calculate the stress on a fault due to some slip on that fault. We also include a topographic free surface. How should we model this surface?</p>
<p>Should we use planar triangles to model a curved fault? What happens at the junction between two triangles? If we do the natural fault thing and require the slip to be tangential to the suface, the slip vector has a step function at the junction between the two triangles. This must be true because the two triangles have different normal vectors and tangent vectors. On the other hand, if we force slip to be continuous across triangles, we have to allow non-tangential slip because at an edge where two triangles meet the slip vector has to be identical but the tangent vectors are different.</p>
<p>That means that we&rsquo;re stuck between a rock and a hard place and have to choose:</p>
<ul>
<li>non-tangential, but continuous slip</li>
<li>tangential, but discontinuous slip.</li>
</ul>
<p>So, planar triangles probably aren&rsquo;t a sustainable or &ldquo;correct&rdquo; way to represent a fault surface unless that surface is perfectly planar or we&rsquo;re zoomed in far enough that the underlying microscale behavior is relevant. Fundamentally the problem is that our fault surface representation is $C^0$, meaning that the function itself is continuous but the derivatives are not. This problem propagates through the whole model up in other ways too. Suppose you choose to have discontinuous, but tangential, slip. Then, there will be stress singularities at every element junction!</p>
<p>Should we use a smoother fault surface representation? Probably. A $C^1$ fault representation where the normal vector is continuous between adjacent elements solves the problem described in the previous paragraph.</p>
<p>Let&rsquo;s back up and consider how to mesh our model earthquake problem from the perspective of <em>truth</em>, <em>accuracy</em> and <em>robustness</em>:</p>
<p><strong>Truth</strong>: The underlying true surface of the Earth is fractal and has sharp corners at basically any zoom level. The fault surfaces are probably similarly fractal and non-smooth.</p>
<p><strong>Accuracy</strong>: There is accurate satellite data on the location of the Earth&rsquo;s surface down to about a a few meters resolution. And for faults, depending on where in the world, the uncertainty in fault location is on the order of 10-10000m. So, only a few digits of accuracy could possibly be relevant in these models since there&rsquo;s already a high level of error introduced by the mere location uncertainty in the faults, much less all the other uncertainties that I haven&rsquo;t discussed here. The main goal, at least at this point in the evolution of the earthquake science field, is in terms of qualitative behavior and questions on the scale of 1-50% or even about the sign of an effect (positive or negative).</p>
<p><strong>Robustness</strong>: This is where smoothness gets really important. From my experience, having smooth boundaries makes so much about integral equation methods simpler. Especially for these faulting/crack problems where hypersingular terms are common.</p>
<h2 id="a-different-approach">A different approach</h2>
<p>My tentative approach that I&rsquo;ve been exploring is:</p>
<ul>
<li>Use at least a $C^1$ mesh. Even smoother surfaces should work fine too. An easy way to get a $C^1$ surface is to use cubic splines. Smooth surfaces make the numerics and code much easier because they remove several types of singularities. This is especially relevant because of the hypersingular integrals from faults that result in stress singularities I described above.</li>
<li>Methods for smooth surfaces tend to be developed by people looking for high-order accuracy. To reduce all forms of error, it makes sense to have accurate representations of both the surfaces involved and the functions defined on that surface. In addition, low order surface discretizations introduce singularities. For example, a boundary element method which is based on $C^0$ elements will have singularities in stress at each element intersection because there&rsquo;s an artificially introduced corner. I don&rsquo;t think the high-order accuracy is particularly important for quasi-static earthquake problems, but in my experience so far these high-order methods tend to work quite well even when the order $p$ is fairly small.</li>
<li>Modifying non-smooth surfaces to be smooth makes a negligible difference in the model results, especially compared to the other uncertainties. And on the scale of a few meters left or right, there is no real reason to think that input geometries are perfectly correct. I think there&rsquo;s some potential for this point to be wrong in some circumstances. For example, if the small scale roughness is extremely important to the problem, or the purpose of the model is to study a sharp corner in a fault. But, in most situations, using smooth surfaces will be both acceptable and easier. If a corner truly is important, I can do some refinement so that the resulting singularity is sufficiently well resolved.</li>
</ul>
<p><em>Thanks to Andreas Klöckner, Brendan Meade and Andrew Bradley for discussions that led to this post. If you want to chat about these kind of topics, please <a href="mailto:t.ben.thompson@gmail.com">get in touch</a>!</em></p>
]]></content:encoded></item><item><title>Cool video of Cascadia earthquake cycle simulations</title><link>https://tbenthompson.com/post/cascadia/</link><pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/cascadia/</guid><description></description><content:encoded> &lt;iframe width="600" height="338" src="https://www.youtube.com/embed/ieN-9MUhND8?start=112" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></content:encoded></item><item><title>130 million triangular dislocations per second with Python/CUDA</title><link>https://tbenthompson.com/post/cutde/</link><pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/cutde/</guid><description> See the GitHub repo</description><content:encoded> &lt;p>&lt;a href="https://github.com/tbenthompson/cutde">See the GitHub repo&lt;/a>&lt;/p></content:encoded></item><item><title>CLUDA - Write once, run anywhere with both CUDA and OpenCL.</title><link>https://tbenthompson.com/post/cluda/</link><pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/cluda/</guid><description> See the GitHub repo</description><content:encoded> &lt;p>&lt;a href="https://github.com/tbenthompson/cluda">See the GitHub repo&lt;/a>&lt;/p></content:encoded></item><item><title>Regularizing rate and state friction for numerical simulation</title><link>https://tbenthompson.com/post/regularized_rate_state/</link><pubDate>Mon, 19 Mar 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/regularized_rate_state/</guid><description> The standard form for rate and state friction produces undefined results for $V = 0$. Some algebra and calculus and microphysics lead to the commonly used regularized form that is still valid at $V=0$. This is based on the presentation in Rice and Ben-Zion 1996 and Lapusta et. al 2000. There&amp;rsquo;s nothing novel here, but the details of this sort of thing are often glossed over in the literature. So, while working it out for myself, I figured I&amp;rsquo;d write it up!</description><content:encoded><![CDATA[ <p>The standard form for rate and state friction produces undefined results for $V = 0$. Some algebra and calculus and microphysics lead to the commonly used regularized form that is still valid at $V=0$. This is based on the presentation in Rice and Ben-Zion 1996 and Lapusta et. al 2000. There&rsquo;s nothing novel here, but the details of this sort of thing are often glossed over in the literature. So, while working it out for myself, I figured I&rsquo;d write it up!</p>
<p>Let&rsquo;s start with the common, empirically-derived rate and state friction with the aging law for state evolution.</p>
<p>\begin{equation}
\tau = \sigma_n (f_0 + a \log(V/V_0) + b \log(V_0\theta/D_c))
\label{rsorig}
\end{equation}</p>
<p>\begin{equation}
\dot{\theta} = 1 - V\theta / D_c
\label{agingorig}
\end{equation}</p>
<p>First, I&rsquo;ll transform to use a somewhat different state variable. $\theta$ is hard to intuitively understand, while $\Psi$ is much more intuitive &ndash; it&rsquo;s just the current absolute offset of the friction coefficient.</p>
<p>\begin{equation}
\Psi = f_0 + b \log(V_0\theta/D_c)
\label{transform}
\end{equation}</p>
<p>or, for the inverse transformation:</p>
<p>\begin{equation}
\theta = \frac{D_c}{V_0}\exp(\frac{\Psi - f_0}{b})
\label{invtransform}
\end{equation}</p>
<p>With this transformation, $\ref{rsorig}$ becomes:</p>
<p>\begin{equation}
\tau = \sigma_n (\Psi + a \log(V/V_0))
\label{rstransform}
\end{equation}</p>
<p>For me, this is nicer in that I can understand the new &ldquo;state&rdquo; parameter, $\Psi$, as the current strength of friction when $V = V_0$.</p>
<p>Now, I&rsquo;ll get the aging law in terms of $\Psi$.  Differentiating \ref{transform}:</p>
<p>\begin{equation}
\dot{\Psi} = b \dot{\theta}/\theta
\end{equation}</p>
<p>and plugging in for $\dot{\theta}$ from \ref{agingorig}:</p>
<p>\begin{equation}
\dot{\Psi} = b/\theta - bV / D_c
\end{equation}</p>
<p>Finally, plug in for $\theta$ from \ref{invtransform}, I get the aging law in terms of $\Psi$.</p>
<p>\begin{equation}
\dot{\Psi} = \frac{bV_0}{D_c}(\exp(\frac{f_0 - \Psi}{b}) - \frac{V}{V_0})
\label{aging01}
\end{equation}</p>
<p>With that done, the second step is to actually do the regularization at $V=0$. Solving $\ref{rstransform}$ for $V$:</p>
<p>\begin{equation}
V = V_0 \exp(\frac{\tau}{a\sigma_n})\exp(-\frac{\Psi}{a})
\end{equation}</p>
<p>The trouble here is that the $\exp(\frac{\tau}{a\sigma_n})$ term can never be 0 and as a result $V=0$ is impossible. Quoting Lapusta et. al 2000:</p>
<blockquote>
<p>A drawback of the logarithmic form (28a) is that the stress is not defined for V = 0. The logarithmic form was derived from purely empirical considerations to match experimental observations (Dieterich, 1979, 1981; Ruina, 1983). However, it has a theoretical basis, in that such a form would result if the direct velocity effect is due to stress biasing of the activation energy in an Arrhenius rate process at contact junctions, at least in the range for which forward microscopic jumps, in the direction of shear stress, are overwhelmingly more frequent than backward jumps.</p>
</blockquote>
<p>So, the $\exp(\frac{\tau}{a\sigma_n})$ term represents the frequency of forward microscopic jumps. What about backward microscopic jumps? They happen too, especially near $V=0$. That suggests adding another term like $-\exp(\frac{-\tau}{a\sigma_n})$ to represent backward microscopic jumps:</p>
<p>\begin{equation}
V = V_0 (\exp(\frac{\tau}{a\sigma_n}) - \exp(\frac{-\tau}{a\sigma_n}))\exp(-\frac{\Psi}{a}) = 2 V_0 \sinh(\frac{\tau}{a\sigma_n})\exp(-\frac{\Psi}{a})
\end{equation}</p>
<p>Solving for $\tau$ and I get the regularized rate-state friction system:</p>
<p>\begin{equation}
\tau = \sigma_n a \sinh^{-1}(\frac{V}{2V_0}\exp(\Psi/a))
\label{rsreg}
\end{equation}</p>
<p>\begin{equation}
\dot{\Psi} = \frac{bV_0}{D_c}(\exp(\frac{f_0 - \Psi}{b}) - \frac{V}{V_0})
\label{aging01-2}
\end{equation}</p>
<p>In my numerical quasidynamic models, I am using equations \ref{rsreg} and \ref{aging01-2}, rather than the more traditional \ref{rsorig} and \ref{agingorig}. Most other rate and state friction implementations (both fully dynamic and quasidynamic) that I have read about also use the regularized forms.</p>
]]></content:encoded></item><item><title>A quasidynamic spring block slider</title><link>https://tbenthompson.com/post/block_slider/</link><pubDate>Sat, 10 Mar 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/block_slider/</guid><description> source available here
Figures first, explanations later!
Recently, I&amp;rsquo;ve been working on some 3D quasidynamic earthquake modeling problems. We&amp;rsquo;re planning to add in some realistic geometries using tectosaur to see what influence that has on the earthquake cycle. While putting together that 3D model, I realized that it consists of two main pieces:
The numerical methods for determining traction on the fault surface given the current values of slip everywhere on the fault (elasticity!</description><content:encoded><![CDATA[ <p><em>source available <a href="https://tbenthompson.com/block_slider.ipynb">here</a></em></p>
<p>Figures first, explanations later!</p>
<img src="https://tbenthompson.com/qd_1d.png" alt="Evolution of a quasidynamic spring-block-slider system."/>
<p>Recently, I&rsquo;ve been working on some 3D quasidynamic earthquake modeling problems. We&rsquo;re planning to add in some realistic geometries using tectosaur to see what influence that has on the earthquake cycle. While putting together that 3D model, I realized that it consists of two main pieces:</p>
<ul>
<li>The numerical methods for determining traction on the fault surface given the current values of slip everywhere on the fault (elasticity!)</li>
<li>The frictional evolution for determining the current velocity on the fault surface from the traction.</li>
</ul>
<p>Or, in pseudocode, it&rsquo;s a simple feedback where I loop:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">while not done:
    slip += velocity * dt
    traction = elastic_solver(slip)
    velocity = rate_state_solver(traction)
</code></pre></div><p>I&rsquo;ve been working on the <code>elastic_solver</code> part for several years (and applying it to a bunch of cool quasistatic problems!) and am quite confident that it&rsquo;s producing the right results. So, when I was running into strange behavior and crazy results, I figured the problems must be in the <code>rate_state_solver</code>.</p>
<p>And the <code>rate_state_solver</code> is actually almost identical in both the 3D problem and a 1D spring block slider problem. So, I set out to repeat the same little exercise that has probably been done a thousand times before.</p>
<p>If you want a much better designed rate-state friction solver, check out <a href="https://github.com/jrleeman/rsfmodel">John Leeman&rsquo;s rsfmodel package</a>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fsolve</span>
<span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">ode</span><span class="p">,</span> <span class="n">odeint</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</code></pre></div><p>First, some reasonably typical material property parameters.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sm</span> <span class="o">=</span> <span class="mf">3e10</span>                  <span class="c1"># Shear modulus (Pa)</span>
<span class="n">density</span> <span class="o">=</span> <span class="mi">2700</span>             <span class="c1"># rock density (kg/m^3)</span>
<span class="n">cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sm</span> <span class="o">/</span> <span class="n">density</span><span class="p">)</span> <span class="c1"># Shear wave speed (m/s)</span>
<span class="n">eta</span> <span class="o">=</span> <span class="n">sm</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">cs</span><span class="p">)</span>        <span class="c1"># The radiation damping coefficient (kg / (m^2 * s))</span>
<span class="n">L</span> <span class="o">=</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">1000</span>              <span class="c1"># Width of our plate boundary (m)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">sm</span> <span class="o">/</span> <span class="n">L</span>                 <span class="c1"># spring constant (Pa / m)</span>
<span class="n">Vp</span> <span class="o">=</span> <span class="mf">1e-9</span>                  <span class="c1"># Rate of plate motion</span>
</code></pre></div><p>Next, the regularized rate and state friction law with the aging law, and some simple parameter choices that lead to cyclic earthquake behavior. I&rsquo;m mixing together a few different sources here, but the main ones are:</p>
<ul>
<li>Erickson and Dunham, <em>An efficient numerical method for earthquake cycles in heterogeneous media: Alternating sub-basin and surface-rupturing events on faults crossing a sedimentary basin, 2014.</em></li>
<li>Segall, <em>Earthquake and Volcano Deformation.</em></li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sigma_n</span> <span class="o">=</span> <span class="mf">50e6</span>   <span class="c1"># Normal stress (Pa)</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">0.015</span>        <span class="c1"># direct velocity strengthening effect</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">0.02</span>         <span class="c1"># state-based velocity weakening effect</span>
<span class="n">Dc</span> <span class="o">=</span> <span class="mf">0.2</span>         <span class="c1"># state evolution length scale (m)</span>
<span class="n">f0</span> <span class="o">=</span> <span class="mf">0.6</span>         <span class="c1"># baseline coefficient of friction</span>
<span class="n">V0</span> <span class="o">=</span> <span class="mf">1e-6</span>        <span class="c1"># when V = V0, f = f0, V is (m/s)</span>

<span class="c1"># Rate-state friction law w/ Rice et al 2001 regularization so that </span>
<span class="c1"># it is nonsingular at V = 0</span>
<span class="k">def</span> <span class="nf">F</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">sigma_n</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arcsinh</span><span class="p">(</span><span class="n">V</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">V0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">state</span> <span class="o">/</span> <span class="n">a</span><span class="p">))</span>
    <span class="c1"># the frictional stress is equal to the friction coefficient * the normal stress.</span>
    <span class="k">return</span> <span class="n">f</span> <span class="o">*</span> <span class="n">sigma_n</span>

<span class="c1"># State evolution law -- aging law.</span>
<span class="k">def</span> <span class="nf">G</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">V0</span> <span class="o">/</span> <span class="n">Dc</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">f0</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">/</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">V</span> <span class="o">/</span> <span class="n">V0</span><span class="p">))</span>
</code></pre></div><p>I&rsquo;ll check $k_{crit}$.</p>
<p>If $k &gt; k_{crit}$, the system is unconditionally unstable and $V \to \infty$.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">kcrit</span> <span class="o">=</span> <span class="n">sigma_n</span> <span class="o">*</span> <span class="n">b</span> <span class="o">/</span> <span class="n">Dc</span> 
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;k/kcrit =&#39;</span><span class="p">,</span> <span class="n">k</span> <span class="o">/</span> <span class="n">kcrit</span><span class="p">)</span>
</code></pre></div><pre><code>k/kcrit = 0.1
</code></pre>
<p>Great!</p>
<p>Next, I&rsquo;ll set up the initial conditions for the model:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x_0</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># slider position</span>
<span class="n">V_slider_0</span> <span class="o">=</span> <span class="n">Vp</span> <span class="o">/</span> <span class="mf">1000.0</span> <span class="c1"># Initially, the slider is moving at 1/1000th the plate rate.</span>
</code></pre></div><p>Setting the initial condition for the state variable is non-trivial because I don&rsquo;t want crazy swings at the beginning of the model. So, I solve for the steady-state state variable given the initial velocity. Since $\frac{\partial state}{\partial t}$ <code>= G(V, state)</code>, that just involves solving for the value of <code>state</code> at which <code>G(V, state) = 0</code>. I use the <code>fsolve</code> scipy function. (Did you notice that equation that was partially Latex and partially code? It felt weird typing that.)</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">steady_state</span><span class="p">(</span><span class="n">V_slider</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">G</span><span class="p">(</span><span class="n">V_slider</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fsolve</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">state_0</span> <span class="o">=</span> <span class="n">steady_state</span><span class="p">(</span><span class="n">V_slider_0</span><span class="p">)</span>
</code></pre></div><p>Okay, so now I&rsquo;m getting to the part where I actually solve the differential equations! But, first, there&rsquo;s an interesting thing about this model. It&rsquo;s actually a differential algebraic equation (DAE) instead of a pure ODE. That&rsquo;s because the current velocity is not defined in differential terms, but instead it is implicity defined as a function of the current shear stress and state parameter. Most of the time, for the 1D spring block slider model that I&rsquo;m looking at here, folks have transformed it into a pure ODE. However, that transformation to an ODE doesn&rsquo;t work very well in the 3D setting. So, for 3D, I&rsquo;m stuck with a DAE. It&rsquo;s not as easy, but it&rsquo;s also not <strong>that</strong> bad. For the sake of easily translating the 1D model here to use a 3D elastic solver, I&rsquo;m going to actually keep using the DAE formulation.</p>
<p>So, at each time step, I need to solve an algebraic equation for the current velocity.</p>
<p>$ \tau_{qs} - F(V, \sigma_n, state) = \eta V$</p>
<p>Aside: This is actually a transformed version of the typical momentum equation $F = ma$, where $ma$ is replaced by the &ldquo;quasidynamic&rdquo; approximation, $\eta V$ and $F$ is replaced by the driving shear force $\tau_{qs}$ and the resisting friction force $F(V, \sigma_n, state)$</p>
<p>As an implementation detail, I warm-start the <code>fsolve</code> call with <code>V_slider_old</code>, which is the velocity from the previous time step - a decent guess for the new velocity.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">current_vel</span><span class="p">(</span><span class="n">tau_qs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">V_slider_old</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tau_qs</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">V</span> <span class="o">-</span> <span class="n">F</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">sigma_n</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fsolve</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">V_slider_old</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><p>When using <code>odeint</code>, I need to define the points in time where I want to observe the state of the system. So, every year, for 15,000 years. (This is not the size of the time step, which is internally decided by <code>odeint</code>).</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">secs_per_year</span> <span class="o">=</span> <span class="mi">365</span> <span class="o">*</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span>
<span class="n">h_t_yrs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">15000.0</span><span class="p">,</span> <span class="mi">15001</span><span class="p">)</span>
<span class="n">h_t</span> <span class="o">=</span> <span class="n">h_t_yrs</span> <span class="o">*</span> <span class="n">secs_per_year</span>
</code></pre></div><p>Next, I define the derivatives.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x_and_state</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">x_and_state</span>
    <span class="c1"># The position of the load point.</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">Vp</span> <span class="o">*</span> <span class="n">t</span> 
    
    <span class="c1"># The extensional force of our spring </span>
    <span class="n">tau_qs</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> 
    
    <span class="c1"># Solve for the current velocity</span>
    <span class="n">V_slider</span> <span class="o">=</span> <span class="n">current_vel</span><span class="p">(</span><span class="n">tau_qs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">V_slider_old</span><span class="p">)</span> 
    
    <span class="c1"># Store the velocity to use it next time for warm-start the velocity solver</span>
    <span class="n">f</span><span class="o">.</span><span class="n">V_slider_old</span> <span class="o">=</span> <span class="n">V_slider</span> 
    
    <span class="n">dxdt</span> <span class="o">=</span> <span class="n">V_slider</span>
    <span class="n">dstatedt</span> <span class="o">=</span> <span class="n">G</span><span class="p">(</span><span class="n">V_slider</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dxdt</span><span class="p">,</span> <span class="n">dstatedt</span><span class="p">]</span>
<span class="n">f</span><span class="o">.</span><span class="n">V_slider_old</span> <span class="o">=</span> <span class="n">V_slider_0</span>
</code></pre></div><p>Define the initial conditions:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">initial_conditions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_0</span><span class="p">,</span> <span class="n">state_0</span><span class="p">])</span>
</code></pre></div><p>And actually solve the equations!</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">history</span> <span class="o">=</span> <span class="n">odeint</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">initial_conditions</span><span class="p">,</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">mxstep</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">h_x</span> <span class="o">=</span> <span class="n">history</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">h_state</span> <span class="o">=</span> <span class="n">history</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_x</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">h_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">h_t</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">h_t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">h_y</span> <span class="o">=</span> <span class="n">h_t</span> <span class="o">*</span> <span class="n">Vp</span>
<span class="n">h_tau_qs</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">h_y</span> <span class="o">-</span> <span class="n">h_x</span><span class="p">)</span>
<span class="n">h_tau_qd</span> <span class="o">=</span> <span class="n">h_tau_qs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">h_V</span>
</code></pre></div><p>Finally, I&rsquo;ll plot up the results.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#plt.style.use(&#39;dark_background&#39;)</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;retina&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;text.usetex&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">18</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.titlesize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;xtick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;ytick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;legend.fontsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.titlesize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;text.latex.preamble&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">usepackage{amsmath}&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;savefig.transparent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">ax11</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_t_yrs</span><span class="p">,</span> <span class="n">h_x</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_t_yrs</span><span class="p">,</span> <span class="n">h_y</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t (yrs)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;position (m)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax12</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_t_yrs</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">h_V</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t (yrs)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">log_{10}(V)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">,</span> <span class="n">sharex</span> <span class="o">=</span> <span class="n">ax11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_t_yrs</span><span class="p">,</span> <span class="n">h_tau_qs</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">tau_{qs}$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_t_yrs</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">h_tau_qd</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">tau_{qd}$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t (yrs)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;shear stress (Pa)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_t_yrs</span><span class="p">,</span> <span class="n">h_state</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t (yrs)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;state&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;qd_1d.pdf&#39;</span><span class="p">,</span> <span class="n">bbox_inches</span> <span class="o">=</span> <span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;qd_1d.png&#39;</span><span class="p">,</span> <span class="n">bbox_inches</span> <span class="o">=</span> <span class="s1">&#39;tight&#39;</span><span class="p">,</span> <span class="n">dpi</span> <span class="o">=</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img src="https://tbenthompson.com/block_slider_files/block_slider_26_0.png" alt="png"></p>
]]></content:encoded></item><item><title>Cloudpickle, serializing functions and monkey patching</title><link>https://tbenthompson.com/post/cloudpickle_monkey_patching/</link><pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/cloudpickle_monkey_patching/</guid><description> I&amp;rsquo;ve been using cloudpickle in the internals of taskloaf for a while since it allows serializing almost all functions and objects. That&amp;rsquo;s really nice since it means I can pass arbitrary functions (tasks, jobs) from one worker to another across the network.
Yesterday, I was curious about the internals of cloudpickle and whether a monkey-patched object would remain patched after being loaded remotely. I read a bit of the source, but figured just trying it was a good idea.</description><content:encoded><![CDATA[ <p>I&rsquo;ve been using <code>cloudpickle</code> in the internals of <a href="https://github.com/tbenthompson/taskloaf">taskloaf</a> for a while since it allows serializing almost all functions and objects. That&rsquo;s really nice since it means I can pass arbitrary functions (tasks, jobs) from one worker to another across the network.</p>
<p>Yesterday, I was curious about the internals of <code>cloudpickle</code> and whether a monkey-patched object would remain patched after being loaded remotely. I read a bit of the source, but figured just trying it was a good idea.</p>
<p>I create a silly, meaningless class and then an instance of that class.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Create a silly class and an object</span>
<span class="k">class</span> <span class="nc">Turkey</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">hi</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&#34;hello&#34;</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">Turkey</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">hi</span><span class="p">())</span>
</code></pre></div><pre><code>hello
</code></pre>
<p>Then, I monkey patch the <code>hi</code> method to return 1 instead of 2. <code>types.MethodType</code> turns a free-standing function into a method that automatically receives the <code>self</code> parameter.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">types</span>
<span class="k">def</span> <span class="nf">hi2</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&#34;SQUAWK&#34;</span>
<span class="n">t</span><span class="o">.</span><span class="n">hi</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">hi2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">hi</span><span class="p">())</span>
</code></pre></div><pre><code>SQUAWK
</code></pre>
<p>First, I&rsquo;ll try <code>pickle</code>. I dump the turkey to a binary blob and reload it.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pickle</span>
<span class="n">blob</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">blob</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t2</span><span class="p">)</span>
</code></pre></div><pre><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-3-21cb01513bb4&gt; in &lt;module&gt;()
      1 import pickle
      2 blob = pickle.dumps(t)
----&gt; 3 t2 = pickle.loads(blob)
      4 print(t2)


AttributeError: 'Turkey' object has no attribute 'hi2'
</code></pre>
<p><code>pickle</code> serialize a reference to the the type of the object and then expects that type to provide all the member functions needed. So, it&rsquo;s not able to handle this monkey patching situation.</p>
<p>Next, I&rsquo;ll try <code>cloudpickle</code>!</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cloudpickle</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">cloudpickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span> <span class="ow">is</span> <span class="n">t2</span><span class="p">)</span>
</code></pre></div><pre><code>False
</code></pre>
<p>Does the <code>hi</code> method remain changed? YES! Thank you, cloudpickle.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">t2</span><span class="o">.</span><span class="n">hi</span><span class="p">())</span>
</code></pre></div><pre><code>SQUAWK
</code></pre>
<p>Ultimately, this makes a lot of sense. <code>cloudpickle</code> just investigates the members of an object (its <code>__dict__</code>) and serializes those. It doesn&rsquo;t need to serialize anything about the generic <code>Turkey</code> class. The key difference with <code>pickle</code> is that <code>cloudpickle</code> has the capability to serialize functions and so it can directly serialize members of the object without reference to its type.</p>
]]></content:encoded></item><item><title>My Python testing set up</title><link>https://tbenthompson.com/post/how_i_test/</link><pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/how_i_test/</guid><description> To follow up on the last post about testing for science and data analytics, I thought it&amp;rsquo;d be nice to talk about the specific tools I use for testing. I don&amp;rsquo;t claim this is the best way to do things, but it tends to work pretty well for small projects and teams.
py.test The py.test tool is handy for running a suite of tests in python. It&amp;rsquo;s a very general tool for running all the tests in a project.</description><content:encoded><![CDATA[ <p>To follow up on the last post about testing for science and data analytics, I thought it&rsquo;d be nice to talk about the specific tools I use for testing. I don&rsquo;t claim this is the best way to do things, but it tends to work pretty well for small projects and teams.</p>
<h2 id="pytest">py.test</h2>
<p>The py.test tool is handy for running a suite of tests in python. It&rsquo;s a very general tool for running all the tests in a project. It can do a lot of powerful things, but requires almost no boilerplate to get started with. The pattern I&rsquo;ve always followed is: Add a <code>tests</code> directory the root of the project. Name any test files in that directory <code>test_xyz.py</code> and then any tests within that file <code>def test_abc():</code>. py.test will find all of these automatically and run them. On several projects that I work on, I have tens or hundreds of tests that, in total, take less than a second to run. As a result, I simply run <code>py.test</code> every time I make a change.</p>
<p>For more happiness, I add a <code>pytest.ini</code> file to the root of my project:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">[pytest]
addopts = -s --tb=short
</code></pre></div><p>The <code>-s</code> causes any text printed by my tests or code to appear on the screen. By default, py.test suppresses printing/logging to standard out. Personally, I don&rsquo;t like that while debugging.
The <code>--tb=short</code> leads to shorter, more concise tracebacks in the case of test failures.</p>
<p>There are a ton of other features available with py.test, like fixtures, fancy assertions and lots of plugins. Here&rsquo;s <a href="http://pythontesting.net/framework/pytest/pytest-introduction/">a more comprehensive intro to py.test</a></p>
<h2 id="golden-master-tests">Golden master tests</h2>
<p>Golden master testing doesn&rsquo;t really <strong>require</strong> any special infrastructure, but it can be nice to have a coordinated way to generate the keys and specify golden master tests. First, I set up a configuration option in <code>tests/conftest.py</code>. This is a special file that <code>py.test</code> uses to load plugins and configuration.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">def pytest_addoption(parser):
    parser.addoption(
        &#34;--save-golden-masters&#34;, action=&#34;store_true&#34;,
        help=&#34;reset golden master benchmarks&#34;
    )
</code></pre></div><p>Then, what you do with this depends on the project. In one project, where I&rsquo;m normally compare numpy arrays, I have a decorator that makes setting up a golden master test really simple.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">def golden_master(digits = 6):
    def decorator(test_fnc):
        try:
            save = pytest.config.getoption(&#34;--save-golden-masters&#34;)
        except AttributeError as e:
            save = False
        @wraps(test_fnc)
        def wrapper(request, *args, **kwargs):
            result = test_fnc(request, *args, **kwargs)
            test_name = request.node.name
            filename = os.path.join(&#39;tests&#39;, &#39;golden_masters&#39;, test_name + &#39;.npy&#39;)
            if save:
                np.save(filename, result)
            correct = np.load(filename)
            np.testing.assert_almost_equal(result, correct, digits)
        return wrapper
    return decorator
</code></pre></div><p>To use this:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">@golden_master(4)
def test_mycomplexfnc(request):
    return mycomplexfnc()
</code></pre></div><p>When I run <code>py.test --save-golden-masters</code>, this will automatically save the result of the function to <code>tests/golden_masters/test_mycomplexfnc.npy</code>. Then, the next time I run <code>py.test</code>, the output of the <code>mycomplexfnc()</code> will be compared to 4 digits against the old saved version.</p>
<p>Modifying this to work with pandas dataframes or other objects would be relatively simple. It could also be refactored to work with general objects without much effort, but, in my case, all the functions where I don&rsquo;t have anything better than a golden master test are functions that return arrays.</p>
<p>Should you put the saved golden master test results in version control? I would say yes, since they are necessary for properly testing the code base.</p>
<h2 id="slow-tests">Slow tests</h2>
<p>Sometimes I have tests that take a little while to run and I don&rsquo;t want them to run every single time I run <code>py.test</code>. So, I add an opt-in option for running slow tests, following the directions <a href="https://docs.pytest.org/en/latest/example/simple.html#control-skipping-of-tests-according-to-command-line-option">on the py.test docs</a>.</p>
<h2 id="pdb--ipdb">pdb + ipdb</h2>
<p>I thought I&rsquo;d just add a note about debuggers, since I&rsquo;m way more productive with a debugger at hand. When my tests fail and I can&rsquo;t figure out why, the first step is to use a debugger to really understand the code. The (P)ython (D)e(b)ugger. Also, the (IP)ython (D)e(b)ugger. These are super handy for dropping into some code to check values/step through the code/general debugging! It combines some of the playfulness of jupyter notebooks with python modules.</p>
<p>Just add these two lines:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">import ipdb
ipdb.set_trace()
</code></pre></div><p>or</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">import pdb
pdb.set_trace()
</code></pre></div><p>And then when those lines are run, execution will stop and I&rsquo;m dropped into a debugging prompt where I can step through the code, examine variable values and other nice things.</p>
]]></content:encoded></item><item><title>Automated testing for scientists and data analysts</title><link>https://tbenthompson.com/post/automated_testing_for_science/</link><pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/automated_testing_for_science/</guid><description><![CDATA[ A lot of scientists and data analysts don&rsquo;t use automatic test suites for verifying their code. And I think that&rsquo;s because it&rsquo;s really hard. Almost all the introductions to automated testing that I&rsquo;ve seen come from a more typical software engineering perspective. They assume you already know exactly what the output of your code should be. And the trouble with science is that that&rsquo;s rarely the case. I mean, if you knew the results ahead of time, you wouldn&rsquo;t be doing science!]]></description><content:encoded><![CDATA[ <p>A lot of scientists and data analysts don&rsquo;t use automatic test suites for verifying their code.
And I think that&rsquo;s because it&rsquo;s really hard.
Almost all the introductions to automated testing that I&rsquo;ve seen come from a more typical software engineering perspective. They assume you already know exactly what the output of your code should be.
And the trouble with science is that that&rsquo;s rarely the case.
I mean, if you knew the results ahead of time, you wouldn&rsquo;t be doing science!</p>
<p>It&rsquo;s hard to square this situation with the consistent loud noises coming from the software development world (in my case, also from inside my head!) saying &ldquo;It might be broken! Write tests! Write tests! Write tests!&rdquo;. &ldquo;But, I can&rsquo;t!&rdquo;</p>
<p>I&rsquo;ve been dealing with this battle for a few years now and feel like I&rsquo;ve figured out some coping strategies.
The simplest solution is to just give up.
And for some types of science, I think that&rsquo;s the right thing to do.
Essentially, whether or not to test is a question of how important it is that the results are correct.
In that sense, I don&rsquo;t think automated testing makes sense for exploratory analysis.
But, for a reusable algorithm or production/final analysis, tests are invaluable.
My work flow mostly involves Python and C++. So, in my case, the question of whether something should be tested often goes hand in hand with whether I&rsquo;m working in a Jupyter notebook or with a proper package.</p>
<p>So, let&rsquo;s back up and ask: why should we have automated tests?</p>
<h2 id="automated-testing">Automated testing</h2>
<p>Automated testing is valuable for ensuring that software does what is is supposed to do. Automated testing:</p>
<ul>
<li>reduces fear when making implementation or design changes (this is huge, in my opinion!) One perspective on when to write tests is to write tests whenever you feel afraid of making a change and to keep writing tests until your not afraid anymore.</li>
<li>enables rapid refactoring without fear of breaking the code base</li>
<li>gives instant feedback</li>
<li>helps improve software accuracy, correctness</li>
<li>provides a form of precise documentation</li>
</ul>
<p><a href="https://stackoverflow.com/questions/67299/is-unit-testing-worth-the-effort">A couple good discussions of why we should unit test.</a></p>
<p>I&rsquo;ve also enjoyed the book &ldquo;Working Effectively with Legacy Code&rdquo; by Michael Feathers, because it focuses on adding tests to previously untested and maybe poorly designed &ldquo;legacy&rdquo; code (which he defines as any code without tests!).
<a href="https://softwareengineering.stackexchange.com/questions/122014/what-are-the-key-points-of-working-effectively-with-legacy-code">A summary of the book.</a></p>
<p>But, returning to the original problem, automated testing can be very difficult in science and analytics because the true answer is not known. Sometimes low level functions can be assigned a &ldquo;correct&rdquo; result. But, for higher level constructs (e.g. an entire time series forecast or the solution of a complex PDE), it&rsquo;s normally much harder to determine what is right and wrong.  It requires careful input from the statistician/scientist/analyst to determine if something looks right or wrong and whether there&rsquo;s anything fishy going on.</p>
<p>So, given that we want to have automated tests, but the inherent nature of the problem makes most testing difficult, what do we do? I still struggle with this issue and am learning all the time, but I&rsquo;ve found three types of approaches particularly useful:</p>
<h2 id="special-cases">Special cases</h2>
<p>This first approach is the most common, the simplest, and often the most valuable.
Suppose I&rsquo;m writing a <a href="https://github.com/tbenthompson/tectosaur">general algorithm for numerically solving elastic partial differential equations for an arbitrary geometry and set of boundary conditions</a>.
It might be really hard to say whether the answer I&rsquo;m getting is correct for
any given problem. But some specific problems have already been solved analytically.
For example, a spherically symmetric problem is easily solved by hand and any number of textbooks
have exact solutions. Or, similarly, the motion of the Earth due to an planar earthquake in an infinite flat Earth has <a href="http://www.bosai.go.jp/study/application/dc3d/DC3Dhtml_E.html">a well known solution</a>.</p>
<p>In some cases, there are very general approaches for building special cases. For example, anyone writing numerical methods for solving PDEs should absolutely learn about the <a href="http://www.personal.psu.edu/jhm/ME540/lectures/VandV/MMS_summary.pdf">method of manufactured solutions</a>, which lets you come up with as many arbitrary test problems as you need.</p>
<p>Unfortunately, even if there are useful special cases, often the resulting tests only validate the program as a whole. What if that program involves thousands of lines of code? I&rsquo;d prefer to be able to test smaller portions of the code base on their own.</p>
<h2 id="property-testing">Property testing</h2>
<p>I think property testing is the most useful approach when the answer isn&rsquo;t known. Instead of check the final answer, we check a <em>property</em> of some final or intermediate part of the calculation. For example:</p>
<ul>
<li>Newton&rsquo;s method for root finding should take exactly one step to solve any linear system of equations. So, a simple property test might be to generate 100 random linear systems and check that all of them are solved in one step. The difference from a special case is that we have a simple algorithm for generating many special cases.</li>
<li>In the case of a finite element or boundary element model, the solution should converge with increasing mesh density.</li>
<li>Running an idempotent function 100 times should leave the system in the same state as running it once.</li>
<li>With an infinite penalty, ridge and lasso regressions return all zeros. With a penalty of zero, ridge and lasso regressions return the same answer as unregularized least squares.</li>
</ul>
<p>I&rsquo;ve always written property tests by hand, but there are some libraries that attempt to automate part of the process. They&rsquo;re probably more useful if you are writing code that might receive inputs from the &ldquo;outside&rdquo; and thus must check behavior in funky edge cases (zero length lists, NaNs, Infinity, etc).
Many property testing libraries are descended from the original QuickCheck Haskell library.
I&rsquo;ve heard good things about <a href="https://github.com/HypothesisWorks/hypothesis-python">Hypothesis</a>.</p>
<h2 id="golden-master-testing">Golden master testing</h2>
<p>Property testing is great, because it links the test back to some fundamental
property of the code/math. The trouble is that it still requires knowing <em>something</em>
about what the code should do. What if you really have no clue what the output
should be, or encoding the properties is too difficult to do in a repeat-able
automated test and requires human input? But you still want to make sure that your refactoring and future development don&rsquo;t break the current implementation.</p>
<p>That&rsquo;s where golden master testing shines. Also known as characterization testing. The idea here is very simple. Choose some input (if that includes randomness, set the random seed manually!). Then, run your function and save the output to a file. Later, to test that the behavior of the function has not changed, run it again on the same input and check that the output matches the previous output exactly.</p>
<p><a href="https://en.wikipedia.org/wiki/Characterization_test">Wikipedia</a> has a good discussion of the advantages and disadvantages of this type of testing. The main two points:</p>
<ul>
<li>it enables testing even if you have no idea what the correct result is. Common in science and analytics. Also, common in legacy systems&hellip;</li>
<li>it only prevents unwanted side effects of software changes, it doesn&rsquo;t actually check that the answer is correct.</li>
</ul>
<h2 id="testing-through-monte-carlo-simulation-a-special-kind-of-property-test-guest-section-by-liz-santorella">Testing through Monte Carlo simulation: A special kind of property test (guest section by Liz Santorella)</h2>
<p>Many statistical algorithms are designed with the following setup in mind:
Assume that data is drawn from some known distribution $D$ with unknown parameters $\theta$.
We don&rsquo;t know $\theta$, but the algorithm $A$ returns $\hat{\theta} \approx \theta$,
with the estimate becoming precise as the data grows large.</p>
<!---
For example,
say we have a series of observations $y_1, y_2, \dots, y_N$ and we assume they are drawn independently from
the Normal$(\mu, \sigma)$ distribution. We don't know $\mu$ or $\sigma$. Applying the algorithm $A$ to the
data should give the unknown parameters, approximately: $A(y_1, y_2, \dots, y_N) \approx (\mu, \sigma)$.
-->
<p>In such a setup, we can make up some parameters, create fake data drawn according
to a distribution with those parameters, and check that, when data is large, the algorithm
returns parameter estimates very close to the parameters you put in.</p>
<p>The appeal of this method is that it allows you to know what the right answer for your
entire procedure is (you made up the data, after all). The downside is that your algorithm
may only work accurately when your data is very large, so the test can take a long time to
run. It can also be hard to tell what counts as a correct answer: If you put in the parameter
$\mu=1$ and recover $\hat{\mu}=1.002$, is the discrepancy becuase your Monte Carlo data is too
small or because your code is wrong?</p>
<p>In practice, I often combine Monte Carlo simulation testing with Golden Master testing.
I run Monte Carlo simulations for several parameter values, including some interesting
edge cases. Then once I&rsquo;m satisfied that the code works, I run it on a very small example,
save both the inputs and the outputs, and make those inputs and outputs into a Golden Master test.
I don&rsquo;t automate the Monte Carlo test itself &ndash; it takes too long to run, and its guarantees
are only probabilistic.</p>
<!---
## Making sure it runs
Sometimes, you don't have time to figure out how to check that your code produces the correct output,
but you can still check that it runs without raising errors. Although this can easily be done in a unit testing
framework, I like to make sure my code runs by creating a Jupyter notebook
that calls almost every line of code that I think is important. I explain why the code behaves
the way it does in the notebook, which then makes pretty good documentation.
-->
]]></content:encoded></item><item><title>Sneaky (transparent) huge pages</title><link>https://tbenthompson.com/post/sneaky-transparent-huge-pages/</link><pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/sneaky-transparent-huge-pages/</guid><description></description><content:encoded><![CDATA[ <p>A week ago, I spent an afternoon on a run in with a sneaky feature of the Linux kernel that made some sparse matrix vector product operations about twice as slow as I expected.</p>
<p>I&rsquo;ve recently learned about using file backed shared memory with mmap to do efficient interprocess communication. At the most basic level, you share data between two processes by writing that data to a file. The file can be made to look a lot like a normal region of memory using <code>mmap</code>, a system call, and can be shared between multiple processes by giving <code>mmap</code> the <code>MAP_SHARED</code> flag. There&rsquo;s a downside though, in that the data has to synced to the hard drive continuously. So, instead of sharing a file that is written to disk, I discovered that you can share a file on a RAM disk &ndash; just a filesystem that is backed by RAM. On Linux, a large <code>tmpfs</code> RAM disk filesystem is often mounted by default at <code>/dev/shm</code>.</p>
<p>Here&rsquo;s some python that creates a NumPy array with <code>mmap</code>. First, let&rsquo;s get set up&hellip;</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mmap</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e7</span><span class="p">)</span>
<span class="n">nb</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">n</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;/dev/shm/abc&#39;</span>
</code></pre></div><p>Next, we need to make sure that the file is the correct size using <code>ftruncate</code>. Finally, we memory map it into a numpy array. <code>np.frombuffer</code> is a handy function when you already have a memory buffer and just want to &ldquo;view&rdquo; it as a numpy array without copying the memory.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w+b&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">ftruncate</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">fileno</span><span class="p">(),</span> <span class="n">nb</span><span class="p">)</span>
    <span class="n">mem</span> <span class="o">=</span> <span class="n">mmap</span><span class="o">.</span><span class="n">mmap</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">fileno</span><span class="p">(),</span> <span class="n">nb</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">mem</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</code></pre></div><p>And for kicks, we&rsquo;ll stick some random numbers in there&hellip;</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">x</span>
</code></pre></div><pre><code>array([ 0.84984349,  0.5928336 ,  0.34455741, ...,  0.45909949,  0.51389915,  0.36287542])
</code></pre>
<p>Looks like everything is working! Right? Okay, so now I&rsquo;m going to create another array of the same size with random indexes into the first array.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
</code></pre></div><p>And then, benchmark an indexing operation on <code>x</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">timeit</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div><pre><code>1.39 s ± 92.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>So, this is where it gets really interesting&hellip; What if we copy <code>x</code> and then do the benchmark again?</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div><pre><code>691 ms ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>When I finally tracked down this performance bug, I was astonished. Why would copying an array make an indexing operation run almost twice as fast?! At first, I did a whole lot of mind-blown I-dont-undestand-this googling including lots of hopelessly nondescript attempts like &ldquo;numpy copy speed up&rdquo; or &ldquo;copy speeds up indexing operation&rdquo;.</p>
<p>Eventually, I tried to think it through. If copying an array makes an operation faster, then by implication, not all memory is made equal. Some memory is faster to access or manipulate than other memory. I was already aware of NUMA (non-uniform memory access), a feature of multi-socket servers that means that cores can access certain chunks of memory several times faster than other &ldquo;further&rdquo; chunks. Maybe <code>mmap</code> is less likely to allocate memory close to the core that needs it, thus causing NUMA issues? But, this copy-speeds-everything-up behavior happens on my laptop, so it can&rsquo;t be a NUMA issue.</p>
<p>I thought the key must be in way I&rsquo;ve created this particular chunk of memory. It&rsquo;s is a bit out of the ordinary to be using mmap. Again, I tried searching all sorts of vague things about &ldquo;mmap memory slower&rdquo; or &ldquo;mmap performance&rdquo;, thinking that maybe something in the way mmap synchronizes to the RAM disk was causing the performance problems. Alas, no one on the internet seemed to be having these problems! At a loss, I just started fiddling around with the mmap flags to see if different options fixed the problem and I discovered a key clue.</p>
<p>Using <code>mmap</code> with the <code>MAP_PRIVATE</code> flag boosted performance of the indexing operation to the same speed as when I copied <code>x</code>.</p>
<p>Around the same point, I had another thought. The current indexes are random. Random indexing of a 50 million element array is bound to cause some caching problems. Does copying speed up a sequential indexing operation? Say, we just skip every other element. Let&rsquo;s try it!</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">idx_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">timeit</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx_seq</span><span class="p">]</span>
</code></pre></div><pre><code>62.8 ms ± 2.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">timeit</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[</span><span class="n">idx_seq</span><span class="p">]</span>
</code></pre></div><pre><code>62.1 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
<p>First off, sequential indexing is much faster because it has much better cache utilization (actually, pretty much ideal cache utilization). I mean it&rsquo;s just a copying operation! This is going to be a memory bandwidth bound operation, so let&rsquo;s calculate really quickly what type of memory bandwidth my little laptop is achieving.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">t_sec</span> <span class="o">=</span> <span class="mf">0.0621</span>
<span class="n">total_bytes</span> <span class="o">=</span> <span class="n">x2</span><span class="o">.</span><span class="n">nbytes</span> <span class="o">+</span> <span class="n">idx_linear</span><span class="o">.</span><span class="n">nbytes</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">nbytes</span>
<span class="k">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">total_bytes</span> <span class="o">/</span> <span class="n">t_sec</span> <span class="o">/</span> <span class="mf">1e9</span><span class="p">),</span> <span class="s1">&#39;GB/s&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>12.882447665056361 GB/s
</code></pre>
<p>Wow! I&rsquo;m pretty sure that&rsquo;s close to the peak memory bandwidth possible from this laptop.</p>
<p>Secondly, the linear memory access is just as fast for the <code>mmap</code>-ed memory as it is for the memory obtained by <code>x.copy()</code> (presumably deep inside numpy there&rsquo;s a <code>malloc</code> call). So, while it&rsquo;s straightforward to say that a random memory access pattern is much slower than a sequential access pattern, there&rsquo;s still the problem of what would make random access slower on some blocks of memory than others. Now, I had something more concrete to start googling&hellip; &ldquo;mmap slow random memory access&rdquo;. And, finally, the first result was my <a href="https://stackoverflow.com/questions/44001260/random-mmaped-memory-access-up-to-16-slower-than-heap-data-access/44152743">salvation from stack overflow</a>.</p>
<p>The random memory access on the <code>mmap</code>-ed memory was slower because each access had to not only load the memory all the way from RAM rather than cache, but also had to load the physical location of that virtual memory from RAM rather than cache. This is because <code>mmap</code>-ed shared memory defaults to using traditional 4kB memory pages while, for most recent Linux installation, <code>malloc</code> default to using 2MB huge pages. The 4kB pages result in far more TLB (translation lookaside buffer) cache misses. Our 400MB chunk of memory would requires 100,000 page table entries with 4kb pages &ndash; way more than fit in the TLB cache.</p>
<p>There was a whole bunch of jargon I didn&rsquo;t define in that last paragraph. Maybe in a future post, I&rsquo;ll go through the details of how virtual memory, the TLB, Huge Pages and Transparent Huge Pages all work. This <a href="https://dzone.com/articles/memory-access-patterns-are">page</a> has some comprehensive descriptions. but just to finish this post out, I&rsquo;ll show some performance counters demonstrating the TLB cache miss problem. The first example is using 4kB pages, while the second is using 2MB huge pages. There are 200x fewer TLB cache misses using huge pages.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">
➜  examples git:(master) ✗ perf stat -e dTLB-load-misses python mmap_bench.py shmem 1
shm square 0.4846489429473877

 Performance counter stats for &#39;python mmap_bench.py shmem 1&#39;:

        19,313,497      dTLB-load-misses                                            

       1.159249907 seconds time elapsed

➜  examples git:(master) ✗ perf stat -e dTLB-load-misses python mmap_bench.py shmemhuge 1
shm square 0.2850306034088135

 Performance counter stats for &#39;python mmap_bench.py shmemhuge 1&#39;:

            91,545      dTLB-load-misses                                            

       0.895894488 seconds time elapsed
</code></pre></div><p>And let&rsquo;s check our original problem by using a separate <code>hugetlbfs</code> RAM disk using huge pages mounted at <code>/mnt/hugepages</code> (remember before we used <code>/dev/shm</code>).</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;/mnt/hugepages/abc&#39;</span>
<span class="n">page_size</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">**</span> <span class="mi">21</span>
<span class="c1"># Using huge pages requires using a number bytes that is an exact multiply of the page size</span>
<span class="n">nb_rounded_to_page_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">page_size</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">nb</span> <span class="o">/</span> <span class="n">page_size</span><span class="p">))</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w+b&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">ftruncate</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">fileno</span><span class="p">(),</span> <span class="n">nb_rounded_to_page_size</span><span class="p">)</span>
    <span class="n">mem</span> <span class="o">=</span> <span class="n">mmap</span><span class="o">.</span><span class="n">mmap</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">fileno</span><span class="p">(),</span> <span class="n">nb_rounded_to_page_size</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">mem</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div><pre><code>657 ms ± 39.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>Hurray! Success!</p>]]></content:encoded></item><item><title>Sparse n-body matrices</title><link>https://tbenthompson.com/post/sparse_nbody/</link><pubDate>Sat, 02 Jul 2016 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/sparse_nbody/</guid><description> This is a sparsity plot of a matrix representing an approximate n-body interaction. The black dots are non-zeros in the matrix. Imagine 2000 stars, each exerting some gravitational pull on each of the other stars. This gravitational interaction could be represented by a dense matrix:
$$ A_{ij} = \frac{G m_i m_j}{|\mathbf{x}_i - \mathbf{x}_j|^2} $$
This works, but dense matrices can be slow when there are many stars. The cost of evaluating a matrix-vector product will scale like $O(n^2)$.</description><content:encoded><![CDATA[ <p><img src="https://tbenthompson.com/images/sparse_treecode_matrix.png" alt="matrix"></p>
<p>This is a sparsity plot of a matrix representing an approximate n-body interaction. The black dots are non-zeros in the matrix. Imagine 2000 stars, each exerting some gravitational pull on each of the other stars. This gravitational interaction could be represented by a dense matrix:</p>
<p>$$
A_{ij} = \frac{G m_i m_j}{|\mathbf{x}_i - \mathbf{x}_j|^2}
$$</p>
<p>This works, but dense matrices can be slow when there are many stars. The cost of evaluating a matrix-vector product will scale like $O(n^2)$. Fortunately, the matrix can be represented in a different way if a little bit of error is okay. The interactions can be separated into those between nearby stars and the remaining interactions between stars that are far from each other. Groups of farfield interactions can be approximated. The result is a matrix that looks like what I&rsquo;m showing above. The matrix has four distinct blocks. The upper left 2000x2000 block of the matrix represents the direct interaction that could not be approximated. The lower left and lower right represent the evaluation of the approximation coefficients. Both are super sparse! The upper right is the evaluation of the influence of the approximation coefficients on the points themselves. This part is less sparse.</p>
<p>Depending on exactly where the matrix coefficients are coming and how the approximation is done, this idea is called a tree-code, the fast multipole method, or a hierarchical matrix and results in an asymptotic complexity of either $O(n)$ or $O(n\log{n})$. The matrix in the figure is too small to show the impressive sparsification achievable with these methods. With a larger problem including 500,000 interacting points, I need &lt;0.1% of the original number of matrix entries (250 billion vs 0.2 billion).</p>
<p>I&rsquo;m using this same idea for elastic interactions in the Earth&rsquo;s crust. The interaction kernel is different, but the idea is fundamentally the same. I&rsquo;m using a specific variant of the fast multipole method called the <a href="http://www.mrl.nyu.edu/~harper/kifmm3d/documentation/publications.html">kernel independent fast multipole method</a>. This is a nice approach since the implementation is the same for each of the interaction kernels I deal with.</p>
]]></content:encoded></item><item><title>"Serializing" a function in C++</title><link>https://tbenthompson.com/post/serialize_fnc_cpp/</link><pubDate>Wed, 23 Dec 2015 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/serialize_fnc_cpp/</guid><description></description><content:encoded> &lt;script src="https://gist.github.com/tbenthompson/94fd4095f89da23dec20.js">&lt;/script></content:encoded></item><item><title>Setting up my website</title><link>https://tbenthompson.com/post/setting_up_website/</link><pubDate>Fri, 11 Dec 2015 00:00:00 +0000</pubDate><guid>https://tbenthompson.com/post/setting_up_website/</guid><description> I have a website now. Since technical stuff is fun, I thought I&amp;rsquo;d share the way I set it up.
The site is a set of statically served HTML pages. Hugo makes this really easy. Hugo is super fast and is written in Go. You can set up a few template pages and then write posts and pages in Markdown. Some layout is also done using Go templates. Running Hugo takes your templates and pages and concerts them into static HTML.</description><content:encoded><![CDATA[ <p>I have a website now. Since technical stuff is fun, I thought I&rsquo;d share the way I set it up.</p>
<p>The site is a set of statically served HTML pages. <a href="https://gohugo.io/">Hugo</a> makes this really easy. Hugo is super fast and is written in Go. You can set up a few template pages and then write posts and pages in Markdown. Some layout is also done using Go templates. Running Hugo takes your templates and pages and concerts them into static HTML. The pages can then be dropped into any web accessible folder and accessed remotely. This is nice, but a few additional steps makes everything even easier:</p>
<ul>
<li><a href="https://pages.github.com/">GitHub Pages</a> serves the pages from a git repository.</li>
<li>The <a href="https://themes.gohugo.io/kiss/">Kiss template</a> provided a nice starting theme. I modified it a bit aiming for a very simple layout and style.</li>
<li>This <a href="https://gohugo.io/hosting-and-deployment/hosting-on-github/">Hugo documentation on using it with GitHub</a> has some good suggestions including a very helpful <code>deploy.sh</code> script that makes deploying changes super easy.</li>
</ul>
<p>You can take a look at all this <a href="https://github.com/tbenthompson/tbenthompson_site">here</a></p>
]]></content:encoded></item></channel></rss>