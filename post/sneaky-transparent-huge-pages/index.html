<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Sneaky (transparent) huge pages | T. Ben Thompson</title><meta name=title content="Sneaky (transparent) huge pages"><meta name=description content><meta name=keywords content><meta property="og:title" content="Sneaky (transparent) huge pages"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://tbenthompson.com/post/sneaky-transparent-huge-pages/"><meta property="article:section" content="post"><meta property="article:published_time" content="2018-01-17T00:00:00+00:00"><meta property="article:modified_time" content="2018-01-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Sneaky (transparent) huge pages"><meta name=twitter:description content><meta itemprop=name content="Sneaky (transparent) huge pages"><meta itemprop=description content><meta itemprop=datePublished content="2018-01-17T00:00:00+00:00"><meta itemprop=dateModified content="2018-01-17T00:00:00+00:00"><meta itemprop=wordCount content="1280"><meta itemprop=keywords content><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{display:block;padding:5px;white-space:pre-wrap;font-size:13px}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}code.has-jax{-webkit-font-smoothing:antialiased;background:inherit!important;border:none!important;font-size:100%}</style><script src=/js/mathjax_setup.js></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>MathJax={}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114592151-1"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-114592151-1")</script><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/syntax.css></head><body><header><nav><a href=/>Home</a>
<a href=/whatido>Work with me</a>
<a href=/post>Posts</a>
<a href=/project>Past Projects</a>
<a href=/book>BIE Tutorials</a></nav></header><main><h2>Sneaky (transparent) huge pages</h2><p><i><time datetime=2018-01-17 pubdate>17 Jan, 2018</time></i></p><content><p>A week ago, I spent an afternoon on a run in with a sneaky feature of the Linux kernel that made some sparse matrix vector product operations about twice as slow as I expected.</p><p>I&rsquo;ve recently learned about using file backed shared memory with mmap to do efficient interprocess communication. At the most basic level, you share data between two processes by writing that data to a file. The file can be made to look a lot like a normal region of memory using <code>mmap</code>, a system call, and can be shared between multiple processes by giving <code>mmap</code> the <code>MAP_SHARED</code> flag. There&rsquo;s a downside though, in that the data has to synced to the hard drive continuously. So, instead of sharing a file that is written to disk, I discovered that you can share a file on a RAM disk &ndash; just a filesystem that is backed by RAM. On Linux, a large <code>tmpfs</code> RAM disk filesystem is often mounted by default at <code>/dev/shm</code>.</p><p>Here&rsquo;s some python that creates a NumPy array with <code>mmap</code>. First, let&rsquo;s get set up&mldr;</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>mmap</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>5e7</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>nb</span> <span class=o>=</span> <span class=mi>8</span> <span class=o>*</span> <span class=n>n</span>
</span></span><span class=line><span class=cl><span class=n>filename</span> <span class=o>=</span> <span class=s1>&#39;/dev/shm/abc&#39;</span>
</span></span></code></pre></div><p>Next, we need to make sure that the file is the correct size using <code>ftruncate</code>. Finally, we memory map it into a numpy array. <code>np.frombuffer</code> is a handy function when you already have a memory buffer and just want to &ldquo;view&rdquo; it as a numpy array without copying the memory.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;w+b&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>ftruncate</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>fileno</span><span class=p>(),</span> <span class=n>nb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mem</span> <span class=o>=</span> <span class=n>mmap</span><span class=o>.</span><span class=n>mmap</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>fileno</span><span class=p>(),</span> <span class=n>nb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>frombuffer</span><span class=p>(</span><span class=n>mem</span><span class=p>,</span> <span class=n>dtype</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>float64</span><span class=p>)</span>
</span></span></code></pre></div><p>And for kicks, we&rsquo;ll stick some random numbers in there&mldr;</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span>
</span></span></code></pre></div><pre><code>array([ 0.84984349,  0.5928336 ,  0.34455741, ...,  0.45909949,  0.51389915,  0.36287542])
</code></pre><p>Looks like everything is working! Right? Okay, so now I&rsquo;m going to create another array of the same size with random indexes into the first array.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=n>n</span><span class=p>)</span>
</span></span></code></pre></div><p>And then, benchmark an indexing operation on <code>x</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span></code></pre></div><pre><code>1.39 s ± 92.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><p>So, this is where it gets really interesting&mldr; What if we copy <code>x</code> and then do the benchmark again?</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x2</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>y</span> <span class=o>=</span> <span class=n>x2</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span></code></pre></div><pre><code>691 ms ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><p>When I finally tracked down this performance bug, I was astonished. Why would copying an array make an indexing operation run almost twice as fast?! At first, I did a whole lot of mind-blown I-dont-undestand-this googling including lots of hopelessly nondescript attempts like &ldquo;numpy copy speed up&rdquo; or &ldquo;copy speeds up indexing operation&rdquo;.</p><p>Eventually, I tried to think it through. If copying an array makes an operation faster, then by implication, not all memory is made equal. Some memory is faster to access or manipulate than other memory. I was already aware of NUMA (non-uniform memory access), a feature of multi-socket servers that means that cores can access certain chunks of memory several times faster than other &ldquo;further&rdquo; chunks. Maybe <code>mmap</code> is less likely to allocate memory close to the core that needs it, thus causing NUMA issues? But, this copy-speeds-everything-up behavior happens on my laptop, so it can&rsquo;t be a NUMA issue.</p><p>I thought the key must be in way I&rsquo;ve created this particular chunk of memory. It&rsquo;s is a bit out of the ordinary to be using mmap. Again, I tried searching all sorts of vague things about &ldquo;mmap memory slower&rdquo; or &ldquo;mmap performance&rdquo;, thinking that maybe something in the way mmap synchronizes to the RAM disk was causing the performance problems. Alas, no one on the internet seemed to be having these problems! At a loss, I just started fiddling around with the mmap flags to see if different options fixed the problem and I discovered a key clue.</p><p>Using <code>mmap</code> with the <code>MAP_PRIVATE</code> flag boosted performance of the indexing operation to the same speed as when I copied <code>x</code>.</p><p>Around the same point, I had another thought. The current indexes are random. Random indexing of a 50 million element array is bound to cause some caching problems. Does copying speed up a sequential indexing operation? Say, we just skip every other element. Let&rsquo;s try it!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>idx_seq</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>idx_seq</span><span class=p>]</span>
</span></span></code></pre></div><pre><code>62.8 ms ± 2.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>y</span> <span class=o>=</span> <span class=n>x2</span><span class=p>[</span><span class=n>idx_seq</span><span class=p>]</span>
</span></span></code></pre></div><pre><code>62.1 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><p>First off, sequential indexing is much faster because it has much better cache utilization (actually, pretty much ideal cache utilization). I mean it&rsquo;s just a copying operation! This is going to be a memory bandwidth bound operation, so let&rsquo;s calculate really quickly what type of memory bandwidth my little laptop is achieving.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>t_sec</span> <span class=o>=</span> <span class=mf>0.0621</span>
</span></span><span class=line><span class=cl><span class=n>total_bytes</span> <span class=o>=</span> <span class=n>x2</span><span class=o>.</span><span class=n>nbytes</span> <span class=o>+</span> <span class=n>idx_linear</span><span class=o>.</span><span class=n>nbytes</span> <span class=o>+</span> <span class=n>y</span><span class=o>.</span><span class=n>nbytes</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>str</span><span class=p>(</span><span class=n>total_bytes</span> <span class=o>/</span> <span class=n>t_sec</span> <span class=o>/</span> <span class=mf>1e9</span><span class=p>),</span> <span class=s1>&#39;GB/s&#39;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>12.882447665056361 GB/s
</code></pre><p>Wow! I&rsquo;m pretty sure that&rsquo;s close to the peak memory bandwidth possible from this laptop.</p><p>Secondly, the linear memory access is just as fast for the <code>mmap</code>-ed memory as it is for the memory obtained by <code>x.copy()</code> (presumably deep inside numpy there&rsquo;s a <code>malloc</code> call). So, while it&rsquo;s straightforward to say that a random memory access pattern is much slower than a sequential access pattern, there&rsquo;s still the problem of what would make random access slower on some blocks of memory than others. Now, I had something more concrete to start googling&mldr; &ldquo;mmap slow random memory access&rdquo;. And, finally, the first result was my <a href=https://stackoverflow.com/questions/44001260/random-mmaped-memory-access-up-to-16-slower-than-heap-data-access/44152743>salvation from stack overflow</a>.</p><p>The random memory access on the <code>mmap</code>-ed memory was slower because each access had to not only load the memory all the way from RAM rather than cache, but also had to load the physical location of that virtual memory from RAM rather than cache. This is because <code>mmap</code>-ed shared memory defaults to using traditional 4kB memory pages while, for most recent Linux installation, <code>malloc</code> default to using 2MB huge pages. The 4kB pages result in far more TLB (translation lookaside buffer) cache misses. Our 400MB chunk of memory would requires 100,000 page table entries with 4kb pages &ndash; way more than fit in the TLB cache.</p><p>There was a whole bunch of jargon I didn&rsquo;t define in that last paragraph. Maybe in a future post, I&rsquo;ll go through the details of how virtual memory, the TLB, Huge Pages and Transparent Huge Pages all work. This <a href=https://dzone.com/articles/memory-access-patterns-are>page</a> has some comprehensive descriptions. but just to finish this post out, I&rsquo;ll show some performance counters demonstrating the TLB cache miss problem. The first example is using 4kB pages, while the second is using 2MB huge pages. There are 200x fewer TLB cache misses using huge pages.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>
</span></span><span class=line><span class=cl>➜  examples git:(master) ✗ perf stat -e dTLB-load-misses python mmap_bench.py shmem 1
</span></span><span class=line><span class=cl>shm square 0.4846489429473877
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> Performance counter stats for &#39;python mmap_bench.py shmem 1&#39;:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        19,313,497      dTLB-load-misses                                            
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       1.159249907 seconds time elapsed
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>➜  examples git:(master) ✗ perf stat -e dTLB-load-misses python mmap_bench.py shmemhuge 1
</span></span><span class=line><span class=cl>shm square 0.2850306034088135
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> Performance counter stats for &#39;python mmap_bench.py shmemhuge 1&#39;:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            91,545      dTLB-load-misses                                            
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       0.895894488 seconds time elapsed
</span></span></code></pre></div><p>And let&rsquo;s check our original problem by using a separate <code>hugetlbfs</code> RAM disk using huge pages mounted at <code>/mnt/hugepages</code> (remember before we used <code>/dev/shm</code>).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>filename</span> <span class=o>=</span> <span class=s1>&#39;/mnt/hugepages/abc&#39;</span>
</span></span><span class=line><span class=cl><span class=n>page_size</span> <span class=o>=</span> <span class=mf>2.0</span> <span class=o>**</span> <span class=mi>21</span>
</span></span><span class=line><span class=cl><span class=c1># Using huge pages requires using a number bytes that is an exact multiply of the page size</span>
</span></span><span class=line><span class=cl><span class=n>nb_rounded_to_page_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>page_size</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>ceil</span><span class=p>(</span><span class=n>nb</span> <span class=o>/</span> <span class=n>page_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;w+b&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>ftruncate</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>fileno</span><span class=p>(),</span> <span class=n>nb_rounded_to_page_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mem</span> <span class=o>=</span> <span class=n>mmap</span><span class=o>.</span><span class=n>mmap</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>fileno</span><span class=p>(),</span> <span class=n>nb_rounded_to_page_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>frombuffer</span><span class=p>(</span><span class=n>mem</span><span class=p>,</span> <span class=n>dtype</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>float64</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span></code></pre></div><pre><code>657 ms ± 39.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><p>Hurray! Success!</p></content><link href=//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css rel=stylesheet type=text/css><div id=mc_embed_signup style="padding:20px 0"><form action="https://tbenthompson.us1.list-manage.com/subscribe/post?u=be25bc8a8ab56f1fb3e6651c2&id=b407942450" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class=validate target=_blank novalidate><div id=mc_embed_signup_scroll><label for=mce-EMAIL style=font-weight:400>If you enjoyed this post, please subscribe!</label>
<input type=email name=EMAIL class=email id=mce-EMAIL placeholder=email@address.com required><div style=display:none><input type=checkbox value=1 name=group[377262][1] id=mce-group[377262]-377262-0 checked>Posts
<input type=checkbox value=2 name=group[377262][2] id=mce-group[377262]-377262-1>BIEBook</div><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_be25bc8a8ab56f1fb3e6651c2_b407942450 tabindex=-1></div><div class=clear><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class=button></div></div></form></div><p></p></main><footer><hr><div class="container has-text-centered"><div class=accounts><a href=/resume.pdf>Resume</a> • <a href="https://scholar.google.com/citations?user=ED9oDz8AAAAJ&hl=en">Google Scholar</a> • <a href=https://github.com/tbenthompson>GitHub</a> • <a href=https://www.linkedin.com/in/ben-thompson-645292125/>LinkedIn</a> • <a href=https://twitter.com/tbenthompson>Twitter</a> • <a href=mailto:t.ben.thompson@gmail.com>Email</a></div><div class=accounts><a href=http://esantorella.com/>My wife is really smart!</a></div><div class=accounts><a href=https://www.instagram.com/wish.and.bean>My dogs have more social media presence than
I do.</a></div></div></footer></body></html>